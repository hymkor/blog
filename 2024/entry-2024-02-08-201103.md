```header
Rem: Alternate-Url: https://zetamatta.hatenablog.com/entry/2024/02/08/201103
Rem: App-Edited: 2024-02-09T01:05:14+09:00
Rem: Draft: yes
Rem: Url-To-Edit: https://blog.hatena.ne.jp/zetamatta/zetamatta.hatenablog.com/atom/entry/6801883189081612314
Rem: Published: 2024-02-08T20:11:03+09:00
Category:
Updated: 2024-02-08T20:11:03+09:00
Title: やばいFTP屋さん（FTPは予想以上に信頼性が低い）
```
### 今までのあらすじ

一応、下で説明しているので読まなくてもよいが、リンクだけ

1. [「rclone cat GIGABYTESFILE.tar.zst | zstd -d」 で Decoding error (36) が出る - 標準愚痴出力](https://zetamatta.hatenablog.com/entry/2023/12/31/184045)
2. [(謎) FTPサイトでダウンロードすると巨大ファイルがよく壊れる - 標準愚痴出力](https://zetamatta.hatenablog.com/entry/2024/02/08/151703)

### 結論

FTP の信頼性は低い。特に回線の太さに比べて、転送ファイルが巨大な時

### 概要

1. レンタルサーバーの 100ギガバイトの容量は FTP でストレージ的に使ってもよいという話なので、マイクラのセーブデータ（3ギガバイト超）を tar | zstd | rclone rcat で定期的にバックアップするようにしてみた。
2. バックアップを rclone cat | zstd -d | tar で照合すると壊れていることが分かる[^verifyarc]
3. rclone cat を curl に置き換えると破損しなくなったので、当初は（rclone のアップロードには問題ないが）rclone のFTPダウンロード処理に不具合があるのかと思われた
4. ところが issue 化するための情報収集で、rclone cat &gt; FILE をやってみると今度は壊れない。一方、curl &gt; FILE だと壊れた  → まったく逆の結果に！

[^verifyarc]: 厳密には tar のところには拙作の[verifyarc](https://github.com/hymkor/verifyarc) というツールで照合していた。

こうなると、FTP というプロトコルの信頼性を疑わざるをえない。それにしても、なんで、FTPでアップロードの過程では別に壊れないのに、ダウンロードの過程では壊れることがあるのか？

### 仮説
#### 【巨大なファイルを遠隔地からダウンロードする時】

ラストワンマイルが遅いからパケットが途中で滞留する → トータルのサイズが小さいと問題ないが、巨大だとスプールしきれなくて、パケットロスや入れ替わりが起こる → ファイルが破損する

#### 【巨大なファイルを遠隔地へアップロードする時】

ファーストワンマイルが遅いが、ローカルファイルなので、今のパケットの送信が終わるまで次のパケットを送らないだけ → パケットのエラーが起こらない → ファイルは別に壊れない

#### 【この仮説の問題点】
それにしたって、普通、エラーくらい検出されるだろ？

### ちょっと ChatGPT 3.5 に聞いてみよう。

**わし**
> FTPで数ギガバイトの巨大なファイルをダウンロードすると、ファイルの中身が壊れることが結構な確率で発生します。原因として考えられることとして何があるでしょうか？

**ChatGTP 3.5**
> FTPで巨大なファイルをダウンロードする際に、ファイルの中身が壊れる可能性がある場合、いくつかの原因が考えられます。以下はそのいくつかの可能性です。
> 
> 1. **ネットワークの不安定性：** ネットワークが不安定である場合、データ転送中にパケットが失われるか、順序が入れ替わる可能性があります。これが発生すると、正確なファイルの転送が妨げられ、ファイルが破損する可能性があります。
> 
> 2. **伝送エラー：** ファイルが送信される際に、伝送エラーが発生することがあります。これは、ハードウェアの問題、通信ラインの問題、またはネットワーク機器の障害によるものです。
> 
> 3. **タイムアウト：** ファイルのダウンロードに時間がかかりすぎ、セッションがタイムアウトする場合、不完全なファイルがダウンロードされる可能性があります。特に、大きなファイルの場合、タイムアウトの設定が重要です。
> 
> 4. **サーバー側の問題：** FTPサーバーが適切に構成されていない、またはサーバー側で問題が発生している場合、ファイルが正しく転送されない可能性があります。
> 
> 解決策としては、以下のいくつかの方法が考えられます。
> 
> - **再試行：** ファイルのダウンロードが破損した場合、再試行することで問題が解決されることがあります。
> - **高速なネットワーク接続：** より高速で安定したネットワーク接続を使用することで、伝送エラーを減少させることができます。
> - **FTPの設定の確認：** FTPクライアントやサーバーの設定を確認し、タイムアウトや再試行の設定などが適切に構成されているか確認してください。
> - **別の転送プロトコルの使用：** FTP以外の転送プロトコル（例：SFTPやHTTP）を検討してみることも一つの選択肢です。
> 
> 問題が解決しない場合は、ネットワークやサーバーの管理者に連絡し、詳細な診断と修正を行うようにお勧めします。

**わし**
> FTP のプロトコルは、チェックサムなどでエラーの検出をしないのでしょうか？

**ChatGPT 3.5**
> FTP（File Transfer Protocol）は、基本的な転送プロトコルの一つで、通常はデータの整合性を確認するための強力なエラーチェックメカニズムを備えていません。FTPは単純な転送メカニズムであり、基本的にはデータの送信と受信を担当しますが、データの誤りや損失の検出・修正には直接的なサポートがありません。
> 
> 主な理由としては、FTPが開発された時期（1970年代）においては、ネットワークの信頼性が今ほど高くなく、エラーチェックのための冗長なデータ送信が一般的でした。その後、より高度なエラーチェックが必要となり、新しいプロトコルやプロトコルの拡張が開発されました。
> 
> 一方で、セキュアなバージョンのFTPであるSFTP（SSH File Transfer Protocol）やFTPの代替として使われるプロトコル、例えばHTTPなどは、通信中のデータ整合性を確保するために異なるメカニズムを採用しています。例えば、TCP（Transmission Control Protocol）のような下位プロトコルが提供する信頼性のあるデータ転送機構を利用したり、ハッシュ関数やチェックサムを使ってデータの整合性を確認することがあります。
> 
> 要するに、FTPが提供するのは基本的な転送機能であり、セキュリティやエラー検出・修正の機能は追加の手段を用いなければなりません。特に大容量のファイルを安全に転送する場合には、SFTPやHTTPS（HTTP over SSL/TLS）など、セキュリティとデータ整合性に対する要件を満たすプロトコルの利用が一般的です。

FTP って http とドッコイドッコイの信頼性だと思ってたんだけど、そうでもないんだ。ChatGTP 3.5 の言うことだから、必ずしも正解だと保証はされてないけど、一応は辻褄はあう。[^t]

[^t]: 「私事だし、もう裏付け調査はいいか」ということで、ここで調査はやめたけど、もしなんか情報あったら一報ください。(追記) 後から考えると、FTP も http と同じく tcp の上に築かれているので、そんな信頼性に大きな差異は無いような気がするんだけど、まぁ、sftp とかに比べると信頼性が落ちるのは確かに間違いない

つまり、今時、FTP を使うヤツが馬鹿。たいへん申し訳ありませんでした。
